\documentclass{beamer}
\usepackage{ctex} %注意这个宏包
\usepackage{color}
\usepackage{graphics,graphicx}
\usetheme{Berkeley}
\useinnertheme{rounded}
\usepackage{pstricks}
\usepackage{pst-plot}
\CTEXoptions[today=old]
\setCJKmainfont[BoldFont={SimHei},ItalicFont={KaiTi}] {WenQuanYi Micro Hei Mono}
\title{Learning HMM Structure for Information Extraction}
\author{Speaker:\\Chunwei Yan}
\institute[PKUSZ]{
    互联网研发中心
}
\date{\today}

\begin{document}
% ------------- title page ----------------------------
%--- the titlepage frame -------------------------%
\begin{frame}
  \titlepage
\end{frame}

\section{Begin}
\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\subsection{Introduction to HMM}
\begin{frame}{Introduction to HMM}
            \pause
\begin{block}{HMM组成 }
    \begin{itemize}
        \item 状态(states)集合 Q \{$q_I, q_0, q_1, \cdots q_F$\}
        \item 状态间的转移(trasitions) $(q \rightarrow q')$
        \item 一个有限的观测(output symbols)集合 $\sum = \{ \sigma_1, \sigma_2, \cdots, \sigma_M\}$
    \end{itemize}
\end{block}

\pause
\begin{block}{模型系数}
\begin{itemize}
    \item 状态转移概率: $P(q \rightarrow q')$
    \item 状态$q$观测为$\sigma$的概率: $P(q \uparrow \sigma)$ 
\end{itemize}
\end{block}
\end{frame}

\subsection{Example}
\begin{frame}{Introduction to HMM}
\begin{block}{Example}
\begin{center}
    \includegraphics[height=150pt]{report5/hmm.png}
\end{center}
\end{block}
\end{frame}

\section{HMM 的使用}
\begin{frame}{HMM 的使用}
    \begin{block}{ 一个HMM模型观测为一个字符串$x$的概率}
    \begin{equation}
    P(x|M) = \sum_{q_1,\cdots,q_l \in Q^t} {
        \prod_{k=1}^{l+1}{
            P(q_{k-1} \rightarrow q_k) P(q_k \uparrow y_k)
        }
    }
    \end{equation}
    \end{block}

    \pause
    \begin{block}{ 如下恢复出最可能生成观测$y$的状态序列}
    %To recover the state sequence $V(x|M)$ that has the highest probability of having produced an observation sequence y:
    \begin{equation}
    P(x|M) = \sum_{q_1,\cdots,q_l \in Q^t} {
        \prod_{k=1}^{l+1}{
            P(q_{k-1} \rightarrow q_k) P(q_k \uparrow y_k)
        }
    }
    \end{equation}
    \end{block}
\end{frame}

\begin{frame}{ 从科技论文头部挖掘信息}
    \begin{block}{论文头}
    \begin{center}
        \includegraphics[width=240pt]{report5/report_header.png}
    \end{center}
    \end{block}
\pause
    \begin{block}{提取的15个类别}
        title, author, affiliation, address, note, email, date\\
        abstract, introduction, phone, keywords, web, degree, publication number(pubnum) and page
         
    \end{block}
\end{frame}

\begin{frame}{ 从科技论文头部挖掘信息}
    %Information Extraction from Research Paper Headers}
    \begin{block}{模型参数学习}
        %Learn Model Parameters}
        可以的两种情况:
        \begin{itemize}
            \pause
            \item 每一个状态(state)代表一种类别(class label)，比如 \textbf{标题}, \textbf{作者}, \textbf{地址}等。
            \pause
            \item 一种类别联系到多种状态，每个状态间仅有有限的转移。
        \end{itemize}
    \end{block}
    
            \pause
    \begin{block}{ 标注新的论文头部}
        \begin{itemize}
            \item 将论文头部的词作为观测值
            \item 用 \textbf{Viterbi 算法}恢复最有可能的状态序列
        \end{itemize}
    \end{block}
\end{frame}

\subsection{从训练集中学习HMM模型}
\begin{frame}{Introduction}
    \textbf{首先要确定模型中有多少状态.}
            \pause
    \begin{block}{可取方案:}
        \begin{itemize}
            \pause
            \item 仅仅为每一个状态分配一种类别
            \pause
            \item 将一种类别联系到多个状态
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{统计方法}
    \begin{block}{ 为取得HMM 系数的最大似然估计}
    \begin{equation}
        \hat{P}(q \rightarrow q') = 
        \frac{c(q \rightarrow q')}
        {\sum_{\sigma \in Q}{c(q \rightarrow s)}}
    \end{equation}

    \begin{equation}
        \hat{P}(q \uparrow \sigma) = 
        \frac{c(q\uparrow \sigma)}
        {\sum_{\rho \in \sum}{c(q \uparrow \rho)}}
    \end{equation}
    \end{block}
\end{frame}

% contents
\begin{frame}{模型学习过程}
    \begin{enumerate}
        \item 为每一个词独立分配一个状态
        \item neighbor-merging(相邻合并)
        \item 进一步合并
            \begin{itemize}
                \item V-merging
                \item M-merging
                \item Bayesian model merging
            \end{itemize}
    \end{enumerate}
\end{frame}

\subsection{一个词一状态}
\begin{frame}{为每一个词独立分配一个状态}
    为训练集中每一个词分配一个独立的状态，同时，当前词汇与下一个词汇间对应着状态转移
    %Each word in the training data is assignd its own state, which transitions to the state of the word that follows it.
            \pause
    \begin{center}
        \includegraphics[height=80pt]{report5/neighbor-1.png}
    \end{center}
\end{frame}

\subsection{Neighbor-merging相邻合并}
\begin{frame}{Neighbor-merging相邻合并}
    %Combines all states that share a transition and have the same class label.    
    合并共享转移(transition)以及相同类别(class)的所有状态
            \pause
    \begin{center}
        \includegraphics[width=250pt]{report5/neighbor-2.png}
    \end{center}
\end{frame}

\subsection{V-merging}
\begin{frame}{V-merging}
    \begin{center}
        \includegraphics[height=150pt]{report5/v-merge-1.png}
    \end{center}
\end{frame}

\begin{frame}{V-merging}
    合并任何有相同类别标签且转移到相同状态或者从共同状态转移的两个状态
            \pause
    \begin{center}
        \includegraphics[height=120pt]{report5/v-merge-2.png}
    \end{center}
\end{frame}

\begin{frame}{V-merging}
    \begin{block}{完整图像}
    \begin{center}
        \includegraphics[width=270pt]{report5/v-merge-3.png}
    \end{center}
    \end{block}
\end{frame}


\subsection{M-merging and Bayesian}
\begin{frame}{M-merging and Bayesian Model Merging}
多个状态对应一个类别:
            \pause
\begin{block}{M-merging}
    在\textbf{neighbor-merging}之后，利用人工的方法迭代地合并状态.    
\end{block}

            \pause
\begin{block}{Bayesian Model Merging}
    利用贝叶斯方法进行HMM的模型状态合并
\end{block}
\end{frame}

\begin{frame}{其它方面}
    \begin{block}{Unlabeled Data(未标注数据)}
        在V-merging 或 M-merging 的基础之上,采用\textbf{Baum-Welch}算法进行非监督学习. 
    \end{block}

            \pause
    \begin{block}{ Distantly-labeled Data(远亲标注数据)}
        为充实训练集,混合使用已经标注的相似数据集. 
        \begin{itemize}
            \item L+D: Labeled 和 Dislabeled 数据集简单组合
            \item L*D: Labeled 和 Dislabeled 数据集的线性插值
        \end{itemize}
    \end{block}
\end{frame}

\section{实验结果}
\begin{frame}{实验数据}
    \begin{center}
        \includegraphics[width=240pt]{report5/traing_set.png}
    \end{center}
\end{frame}

\begin{frame}{一个类别一个状态的实验结果}
    \begin{center}
        \includegraphics[width=240pt]{report5/one_state_per_class.png}
    \end{center}
\end{frame}

\begin{frame}{M-merging V-Merging结果}
    \begin{center}
        \includegraphics[width=240pt]{report5/m-picture.png}
    \end{center}
\end{frame}

\begin{frame}{M-merging V-Merging结果}
    \begin{center}
        \includegraphics[width=240pt]{report5/m-picture2.png}
    \end{center}
\end{frame}

\begin{frame}{Baum-Welch 后期加工结果}
    \begin{center}
        \includegraphics[width=240pt]{report5/bw.png}
    \end{center}
\end{frame}

\section{Conclude}
\begin{frame}{Conclude}
\begin{enumerate}
            \pause
    \item HMM 在信息提取领域真的有潜力, 这篇论文中达到了 92.9\%的精度.
            \pause
    \item 一个类别(class)对应一到多个状态(state)更加有效
            \pause
    \item 远亲标注(Distantly-labeled)数据在模型系数估计方面有很大帮助,插值组合
\end{enumerate}
\end{frame}

\begin{frame}{References}
\begin{thebibliography}{10}
\bibitem{hmm} [Kristie .S, Andrew .M and Ronald .R, 1999]
    \newblock \emph{Learning Hidden Markov Model Structure for Information Extraction}
    \newblock School of Computer Science Carneie Mellon University
    \newblock Just Research
\end{thebibliography}
\end{frame}
\end{document}
